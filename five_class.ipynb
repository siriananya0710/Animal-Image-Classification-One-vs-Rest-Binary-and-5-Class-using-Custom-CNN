{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: IITGN Internship Qualification Task\n",
        "---"
      ],
      "id": "6309d3ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Project 5: ML for Sustainability: Satellite Data Processing for Detecting Pollution Sources \n",
        "\n",
        "#### -----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "### Five class Classifiction\n"
      ],
      "id": "07b96713"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# importing the required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
      ],
      "id": "5cc75f0f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the paths\n",
        "source_dir = 'dataset2'\n",
        "target_dir = 'split2'\n",
        "\n",
        "# Create target directories if they don't exist\n",
        "for split in ['train', 'test', 'val']:\n",
        "    os.makedirs(os.path.join(target_dir, split, 'antelope'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, split, 'badger'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, split, 'cat'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, split, 'dolphin'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(target_dir, split, 'elephant'), exist_ok=True)\n",
        "\n",
        "# Function to split data and move files\n",
        "def split_data(source_dir, target_dir, class_folder):\n",
        "    files = os.listdir(os.path.join(source_dir, class_folder))\n",
        "    random.shuffle(files)  # Shuffle the list of files randomly\n",
        "    train_files, test_val_files = train_test_split(files, test_size=0.25, random_state=42)\n",
        "    test_files, val_files = train_test_split(test_val_files, test_size=0.4, random_state=42)\n",
        "\n",
        "    for file in train_files:\n",
        "        shutil.copy(os.path.join(source_dir, class_folder, file), os.path.join(target_dir, 'train', class_folder))\n",
        "    for file in test_files:\n",
        "        shutil.copy(os.path.join(source_dir, class_folder, file), os.path.join(target_dir, 'test', class_folder))\n",
        "    for file in val_files:\n",
        "        shutil.copy(os.path.join(source_dir, class_folder, file), os.path.join(target_dir, 'val', class_folder))\n",
        "\n",
        "# Perform the split for raccoon and non_raccoon classes\n",
        "split_data(source_dir, target_dir, 'antelope')\n",
        "split_data(source_dir, target_dir, 'badger')\n",
        "split_data(source_dir, target_dir, 'cat')\n",
        "split_data(source_dir, target_dir, 'dolphin')\n",
        "split_data(source_dir, target_dir, 'elephant')"
      ],
      "id": "7ab849ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Function to resize images\n",
        "def resize_images_in_directory(directory, target_size=(150, 150)):\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    with Image.open(file_path) as img:\n",
        "                        resized_img = img.resize(target_size)\n",
        "                        resized_img.save(file_path)\n",
        "                        print(f\"Resized {file} to {target_size}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error resizing {file}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "directory_path = [\"split2/test\", \"split2/val\", \"split2/train\"]\n",
        "for i in directory_path:\n",
        "    resize_images_in_directory(i)"
      ],
      "id": "e241d4bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Augmentation\n"
      ],
      "id": "48ab0434"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, shutil, random\n",
        "from turtle import mode\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tqdm import tqdm\n",
        "\n",
        "source_dir = 'split2/train/elephant'\n",
        "\n",
        "augmented_dir = 'split2/train/elephant/augmented'\n",
        "os.makedirs(augmented_dir, exist_ok=True)\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=128./255,              # Rescale pixel values to [0, 1]\n",
        "    brightness_range=[0.5, 1.5],   # Random brightness adjustment\n",
        "    zoom_range=[0.8, 1.2],         # Random zooming\n",
        "    fill_mode='nearest',           # Fill mode for filling pixels after augmentation\n",
        "    horizontal_flip=True,          # Random horizontal flipping\n",
        "    vertical_flip=True,            # Random vertical flipping\n",
        "    rotation_range=30,             # Random rotations\n",
        "    shear_range=0.3,               # Random shearing\n",
        ")\n",
        "\n",
        "image_files = [i for i in os.listdir(source_dir) if i.endswith('.jpg')]\n",
        "print(len(image_files))\n",
        "\n",
        "target_count = 255"
      ],
      "id": "81dc4ff6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Loop until the target count is reached\n",
        "generated_count = 0\n",
        "print(f'Generating {target_count} augmented images...', end='\\n\\n')\n",
        "with tqdm(total=target_count, desc=\"Augmentation Progress\") as pbar:\n",
        "    while generated_count < target_count:\n",
        "        # Randomly select an image from the original dataset\n",
        "        image_file = np.random.choice(image_files)\n",
        "        image_path = os.path.join(source_dir, image_file)\n",
        "\n",
        "        # Load the image\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Ensure it's in RGB format\n",
        "\n",
        "        # Generate augmented images using the data generator\n",
        "        aug_iter = datagen.flow(np.expand_dims(image, axis=0), batch_size=1)\n",
        "        augmented_images = [next(aug_iter)[0].astype(np.uint8) for _ in range(5)]  # Create 5 augmented images per original\n",
        "\n",
        "        # Save the augmented images\n",
        "        for i, aug_image in enumerate(augmented_images):\n",
        "            aug_filename = f'augmented_{generated_count + i}.jpg'\n",
        "            aug_image_path = os.path.join(augmented_dir, aug_filename)\n",
        "            cv2.imwrite(aug_image_path, cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Update the count of generated images\n",
        "        generated_count += len(augmented_images)\n",
        "        # if generated_count % 6000 == 0:\n",
        "        #     print(f'{generated_count} images generated...')\n",
        "        pbar.update(len(augmented_images))\n",
        "\n",
        "print(f'Generated {generated_count} augmented images.')"
      ],
      "id": "7000c0ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Splitting\n"
      ],
      "id": "0a09f5e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Loading dataset\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def create_dataset_from_directory(directory):\n",
        "    x = []\n",
        "    y = []\n",
        "    for class_label, class_name in enumerate(['antelope', 'badger', 'cat', 'dolphin', 'elephant']):\n",
        "        class_dir = os.path.join(directory, class_name)\n",
        "        for file in os.listdir(class_dir):\n",
        "            if file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                file_path = os.path.join(class_dir, file)\n",
        "                try:\n",
        "                    with Image.open(file_path) as img:\n",
        "                        # Note: No resizing here, only loading and normalization\n",
        "                        img = np.array(img) / 255.0  # Normalize pixel values\n",
        "                        x.append(img)\n",
        "                        y.append(class_label)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file}: {e}\")\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# Define paths\n",
        "train_dir = 'split2/train'\n",
        "test_dir = 'split2/test'\n",
        "val_dir = 'split2/val'\n",
        "\n",
        "# Load and preprocess images\n",
        "x_train, y_train = create_dataset_from_directory(train_dir)\n",
        "x_test, y_test = create_dataset_from_directory(test_dir)\n",
        "x_val, y_val = create_dataset_from_directory(val_dir)\n",
        "y_train_encoded = to_categorical(y_train, num_classes=5)\n",
        "y_val_encoded = to_categorical(y_val, num_classes=5)\n",
        "y_test_encoded = to_categorical(y_test, num_classes=5)\n",
        "# Print shapes of datasets\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "print(\"x_val shape:\", x_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "\n",
        "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
        "print(\"y_val_encoded shape:\", y_val_encoded.shape)\n",
        "print(\"y_test_encoded shape:\", y_test_encoded.shape)"
      ],
      "id": "7c82231e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Custom CNN\n"
      ],
      "id": "396d1114"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Layer 1: Convolutional layer with 3x3 kernel and 64 filters\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "\n",
        "# Layer 2: MaxPooling layer\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Layer 3: Convolutional layer with 5x5 kernel and 128 filters\n",
        "model.add(Conv2D(128, (5, 5), activation='relu'))\n",
        "\n",
        "# Layer 4: MaxPooling layer\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Layer 5: Convolutional layer with 7x7 kernel and 256 filters\n",
        "model.add(Conv2D(256, (7, 7), activation='relu'))\n",
        "\n",
        "# Layer 6: MaxPooling layer\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Layer 7: Convolutional layer with 512 filters\n",
        "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
        "\n",
        "# Layer 8: MaxPooling layer\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flatten the output before feeding into the fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Layer 9: Fully connected layer with 512 units\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "# Layer 10: Output layer with 5 units for 5 classes\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train_encoded,\n",
        "                    epochs=50,\n",
        "                    validation_data=(x_val, y_val_encoded))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test_encoded)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Save the model as 'new_model'\n",
        "model.save('new_model.h5')"
      ],
      "id": "63563f33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Get predictions\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convert one-hot encoded labels to categorical labels\n",
        "y_true = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4'], yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "id": "983ac2cc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}